{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MC_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPEbCHaZcfhITF2i7kgNqyn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmsk0711/RL/blob/main/MC_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfNXp3Dzrt0-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "    \n",
        "    def step(self, a):\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a==0:\n",
        "            self.move_left()\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1  # 보상은 항상 -1로 고정\n",
        "        done = self.is_done() \n",
        "        return (self.x, self.y), reward, done #  상태, 보상, 끝남\n",
        "\n",
        "    def move_left(self):\n",
        "        if self.y==0:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==5 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1\n",
        "\n",
        "    def move_right(self):\n",
        "        if self.y==1 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        elif self.y==6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "      \n",
        "    def move_up(self):\n",
        "        if self.x==0:\n",
        "            pass\n",
        "        elif self.x==3 and self.y==2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self):\n",
        "        if self.x==4:\n",
        "            pass\n",
        "        elif self.x==1 and self.y==4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x+=1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x==4 and self.y==6: # 목표 지점인 (4,6)에 도달하면 끝난다\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "      \n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)\n",
        "\n",
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) # q벨류를 저장하는 변수. 모두 0으로 초기화. \n",
        "        self.eps = 0.9 \n",
        "        self.alpha = 0.01\n",
        "        \n",
        "    def select_action(self, s):\n",
        "        # eps-greedy로 액션을 선택\n",
        "        x, y = s\n",
        "        coin = random.random() # 0이하 랜덤값\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0,3) # 0,1,2,3\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:] \n",
        "            print(action_val) # # [-188.21538193 -161.20099035 -167.55918539 -104.58049598]\n",
        "            action = np.argmax(action_val) # 위의 나온값들의 최댓값 인덱스\n",
        "        return action\n",
        "\n",
        "    def update_table(self, history):\n",
        "        # 한 에피소드에 해당하는 history를 입력으로 받아 q 테이블의 값을 업데이트 한다\n",
        "        cum_reward = 0\n",
        "        for transition in history[::-1]:\n",
        "            s, a, r, s_prime = transition\n",
        "            x,y = s\n",
        "            # 몬테 카를로 방식을 이용하여 업데이트.\n",
        "            self.q_table[x,y,a] = self.q_table[x,y,a] + self.alpha * (cum_reward - self.q_table[x,y,a]) # 상태 + 알파*(보상-현재상태)\n",
        "            cum_reward = cum_reward + r \n",
        "\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.03\n",
        "        self.eps = max(self.eps, 0.1)\n",
        "\n",
        "    def show_table(self):\n",
        "        # 학습이 각 위치에서 어느 액션의 q 값이 가장 높았는지 보여주는 함수\n",
        "        # print(self.q_table)\n",
        "        q_lst = self.q_table.tolist()\n",
        "        # print(\"B:\",q_lst)\n",
        "        data = np.zeros((5,7))\n",
        "        for row_idx in range(len(q_lst)):\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)\n",
        "      \n",
        "def main():\n",
        "    env = GridWorld()\n",
        "    agent = QAgent()\n",
        "\n",
        "    for n_epi in range(1000): # 총 1,000 에피소드 동안 학습\n",
        "        done = False\n",
        "        history = []\n",
        "\n",
        "        s = env.reset()\n",
        "        while not done: # 한 에피소드가 끝날 때 까지\n",
        "            # print(s) # (3, 0) (3, 1) (4, 1)\n",
        "            a = agent.select_action(s)\n",
        "            # print(a) # 1,2,3,0\n",
        "            s_prime, r, done = env.step(a)\n",
        "            # print(s_prime,r,done) # (3, 0) -1 False, (3, 1) -1 False, (3, 2) -1 False\n",
        "            history.append((s, a, r, s_prime))\n",
        "            s = s_prime\n",
        "        agent.update_table(history) # 히스토리를 이용하여 에이전트를 업데이트\n",
        "        agent.anneal_eps()\n",
        "\n",
        "    agent.show_table() # 학습이 끝난 결과를 출력\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OR6LIVRzuU8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a= random.random()\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8iUXqXEwp9Z",
        "outputId": "074e4e0b-f42c-4e0b-c7c6-3ed8ec4b67a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4413328294889357"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6IPY_nIp0Ogp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}